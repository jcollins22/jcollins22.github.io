---
layout: post
title:      "Project 1: Data Cleaning and Feature Engineering"
date:       2019-06-01 01:42:12 +0000
permalink:  project_1_data_cleaning_and_feature_engineering
---


In the module 1 project, we were to take a semi-organized, slightly dirty, intentionally altered, slightly confusing, and partially ambiguous data file regarding prices of homes in Kings County. We were then told to use the skills we had procured in the first few weeks of our course and create a multivariate regression model to predict house price.  Though many steps are involved in making a well working model, arguable the most important is data cleaning and feature engineering. I list these together because they both involve transformations to the data in terms of individual values and types as well as in terms of dimensionality of the data set. 

When attempting to clean this data, one of the main problems encountered was the column specifying the basement square foot measurement. There were 2 inherent problems to this column that negatively fed each other. First, the data was encoded as string variables even though the values themselves were actually typed as float with one exception. There was a placeholder value of “?” for some houses. The existence of this caused the entire column to be read as a string when imported from the .csv file. First the value of “?” had to be replaced with some number and then the column could be converted to float. I replaced the “?” with 0.0 because from a modeling standpoint if we cannot specify or advertise the square footage of the basement, then it is unlikely to be increasing the selling price of the home. Once this was done and the type was reassigned to float, the only other problem with this column is that normal data transforms (specifically log transforms) wouldn’t work because of the values were 0. 

Then came the feature engineering part. The difficulty here was to create features that would not be multicollinear with other columns or would at least be better predictors if they were. The easiest of these was to combine “sqft_above” and “sqft_basement” into a comprehensive “sqft_total.” Other features I created were ration of bathrooms to bedrooms, percentage of total lot used, age of the house, as well as a Boolean value for the following: waterfront lot, basement, and renovated. Additionally, I log transformed all sqft variables except basement and age of house. I applied a min/max scaling to age of house. I then created 1-hot encoding variables for the following: view, condition, grade, bedrooms, bathrooms, and floors. I did this because though these variables are numerical values they are not continuous variables like say price (our target variable) or square footage. Instead they are ordinal values where the difference between 1 and 2 is not by definition the same as the difference between 2 and 3. For this reason, expanding them into dummy variables as categories made more sense. In keeping with best practices, when I did this I dropped the first dummy column to remove multicollinearity. 

The most interesting feature I engineered was a binning of zip code by median salary range. I looked up the list of the 20 wealthiest zip codes in Kings County and then created a binning system to separate the given zip codes into high, mid-high, mid, and other. Since real estate is often espoused to be about location, location, location, I figured that this binning of area by zip code would help with the prediction. 

When I have time to reexamine this project as I learn more, I plan to investigate if any of the features I engineered actually detracted from the efficacy of the model. At this time I am not yet able to fully determine that to a level I would trust. Further I plan to investigate a little more what the optimal cutoff for excluding one variable out of variables with high correlation coefficients between them. I have a suspicion that by using an arbitrary .7 or .75 we may actually be losing out on predictive power in our models. Lastly, when I have a better grasp of different selection algorithms, I plan to use different selection algorithms to see if I can better my model. Once I use these I can then compare those models to other models for prediction that we will learn about later in the course and hopefully determine some criteria to help predict which type of analytic technique will perform best for a given situation, dataset, and prediction problem. 

